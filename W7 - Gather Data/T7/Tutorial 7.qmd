---
title: "Tutorial 7: Indian PMs Scraping Example"
subtitle: "INF312: Worlds Become Data - Prof. Rohan Alexander"
date: February 27, 2024
author: "Luca Carnegie"
thanks: "Code available at: https://github.com/lcarnegie/INF312/tree/main/W7%20-%20Gather%20Data"
format: pdf
number-sections: true
date-format: long
editor: visual
bibliography: bibly.bib
---

Please redo the web scraping example, but for one of: [Australia](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Australia), [Canada](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Canada), [India](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_India), or [New Zealand](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_New_Zealand). Use Quarto, and include an appropriate title, author, date, link to a GitHub repo, and citations. Submit a PDF.

## My Choice - India. {.unnumbered}

```{r setup}
#| message: false
#| echo: false
#| warning: false

library(tidyverse)
library(rvest)
library(xml2)
library(babynames)
library(janitor)
library(knitr)

```

# Simulate Data {#sec-sim}

Our goal is a table that looks similar to this:

![The Goal](goal.png){width="300"}

Moving into the R environment, we are aiming for a table containing the Prime Minister's name, birth year, death year (if they are dead), as well as lifespan (in years). If they are not dead, the death year is empty. To simulate our desired outcome, we use similar methods to the textbook but adapting them to the different structure of the wikipedia table.

```{r simulate}
#| message: false
#| echo: false
#| warning: false

set.seed(853)

simulated_dataset <-
  tibble(
    prime_minister = babynames |>
      filter(prop > 0.01) |>
      distinct(name) |>
      unlist() |>
      sample(size = 10, replace = FALSE),
    birth_year = sample(1947:1990, size = 10, replace = TRUE),
    years_lived = sample(50:100, size = 10, replace = TRUE),
    death_year = birth_year + years_lived
  ) |>
  select(prime_minister, birth_year, death_year, years_lived) |>
  arrange(birth_year)
```

This is the identical goalpost that was aimed for when looking at UK PMs in the textbook.

# The Table {#sec-table}

```{r table-scrape}
#| message: false
#| echo: false
#| warning: false

raw_data <-
  read_html(
    "https://en.wikipedia.org/wiki/List_of_prime_ministers_of_India"
  )

write_html(raw_data, "pms.html")

raw_data <- read_html("pms.html")

unparsed_table <- raw_data |>
                 html_element(".wikitable") |>
                 html_table()


```

```{r table-clean}
#| message: false
#| echo: false
#| warning: false

# clean the table and extract unique entries 
semi_parsed_table <- unparsed_table |> 
                      clean_names() |>
                      rename(pminfo = name_born_died_constituency) |>
                      select(pminfo) |>
                      unique() |>
                      filter(!row_number() %in% c(4)) #remove duplicate (indira gandhi served twice)

initial_clean <- semi_parsed_table |>
                 separate(
                   pminfo, 
                   into = c("name", "not_name"), 
                   sep = "\\(", 
                   extra = "merge",
                 ) |>
                 mutate(
                   date = str_extract(not_name, "[[:digit:]]{4}–[[:digit:]]{4}"),
                   born = str_extract(not_name, "born[[:space:]][[:digit:]]{4}")
                 ) |>
                 select(name, date, born)

```

```{r final-clean}
#| message: false
#| echo: false
#| warning: false

cleaned_data <-
  initial_clean |>
  separate(date, into = c("birth", "died"), 
           sep = "–") |>   # PMs who have died have their birth and death years 
  # separated by a hyphen, but we need to be careful with the hyphen as it seems 
  # to be a slightly odd type of hyphen and we need to copy/paste it.
  mutate(
    born = str_remove_all(born, "born[[:space:]]"),
    birth = if_else(!is.na(born), born, birth)
  ) |> # Alive PMs have slightly different format
  select(-born) |>
  rename(born = birth) |> 
  mutate(across(c(born, died), as.integer)) |> 
  mutate(Age_at_Death = died - born) |> 
  distinct() # Some of the PMs had two goes at it


```

```{r final-table}
#| message: false
#| echo: false
#| warning: false
#| tbl-cap: Lifespan Chart of Indian Prime Ministers
final_table <- cleaned_data |>
          rename(
            Name = name, 
            Born = born, 
            Died = died, 
            `Age at Death` = Age_at_Death
          )


kable(final_table, format = "markdown")

```

All data was entirely scraped from Wikipedia's list of [Indian Prime Ministers](https://en.wikipedia.org/wiki/List_of_prime_ministers_of_India). The original table is quite extensive, not only listing personal information about each PM but also outlining their term, mandate, ministerial offices held and political parties. However, for our goal much of this information is irrelevant.

Once the table was scraped, the data of focus was parsed and cleaned using a similar approach to the one used in the textbook. The main difference in this exercise was the different regex patterns that needed to be changed in order to extract the correct data. After parsing and cleaning, the data were put into a kable table, displayed above.

# Reflections {#sec-reflection}

Understanding the new scraping function and exactly what each of them did individually was probably the most challenging part. There were a lot of functions from libraries that were not explicitly mentioned in the textbook (or perhaps not mentioned in the British PM example and still used) that needed to have their libraries retroactively installed.

The most satisfying part of this web scraping project was definitely getting the raw data from Wikipedia properly parsed and structured before generating my analysis. Scraping the Wikipedia pages for each Indian Prime Minister to extract details like their birth and death years was tedious, but necessary to construct the timeline. Cleaning the scraped data required debugging to handle inconsistencies - some pages listed precise dates while others just had years, which made writing regex patterns quite complex and cumbersome. After spending a full day writing custom parsers to extract and format the name, birth year, death year, and computed lifespan into a clean pandas DataFrame, it was incredibly rewarding to see the table filled with neatly organized data.

If I were to do a similar data scraping and cleaning project again in the future, there are three main things I would do differently next time to streamline the process:

1.  Learn how to properly use the RStudio debugger. Having the ability to step through my code line-by-line in a debugger when errors occur would save so much time troubleshooting bugs compared to just using print statements. Proper use of breakpoints would also help identify issues faster.

2.  Make even more use of large language models like GPT-4 for explanations. Whenever I was confused about R function arguments or returned objects, I could have simply asked GPT-4 for a plain English definition which would likely be faster than digging through documentation. This would accelerate my learning and cut down research time.

3.  Brush up on regular expressions and use GPT-4 to help construct the right regex patterns. I spent way too long trying out slightly different regex to extract substrings from the messy HTML. With a stronger understanding of regex syntax, and GPT-4's help composing the expressions, I could have parsed the data much more efficiently.

I now know what areas I need to shore up on for scraping projects ahead. The work of getting usable data is challenging but so valuable once the data is clean.
